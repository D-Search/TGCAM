# test_tesla_gpu.py
import torch
import os

def test_tesla_p40():
    """ä¸“é—¨æµ‹è¯•Tesla P40 GPU"""
    print("ğŸ§ª Tesla P40 GPUæµ‹è¯•")
    
    # å…³é”®ç¯å¢ƒå˜é‡
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
    
    print("âœ… ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")
    print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
    
    # æ£€æŸ¥CUDA
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"  å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
        
        # æµ‹è¯•è®¡ç®—
        print("\nğŸš€ æµ‹è¯•GPUè®¡ç®—...")
        device = torch.device('cuda:0')
        x = torch.randn(5000, 5000).to(device)
        y = torch.randn(5000, 5000).to(device)
        
        import time
        start = time.time()
        z = x @ y  # çŸ©é˜µä¹˜æ³•
        torch.cuda.synchronize()
        end = time.time()
        
        print(f"âœ… GPUè®¡ç®—æµ‹è¯•é€šè¿‡!")
        print(f"   è®¡ç®—æ—¶é—´: {end - start:.3f}ç§’")
        
        # æµ‹è¯•å¤šGPU
        if torch.cuda.device_count() > 1:
            print("\nğŸ”„ æµ‹è¯•å¤šGPUæ•°æ®å¹¶è¡Œ...")
            model = torch.nn.Linear(1000, 1000)
            model = torch.nn.DataParallel(model, device_ids=[0, 1])
            model = model.to(device)
            print("âœ… å¤šGPUæ•°æ®å¹¶è¡Œæµ‹è¯•é€šè¿‡!")
        
    else:
        print("âŒ CUDAä¸å¯ç”¨")

if __name__ == '__main__':
    test_tesla_p40()
